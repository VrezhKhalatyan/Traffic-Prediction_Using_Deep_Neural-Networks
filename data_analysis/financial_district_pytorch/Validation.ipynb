{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from dataset import trafficDataLoader\n",
    "from model import GRNN\n",
    "from utils import Log\n",
    "\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data . . .\n",
      "Done loading data . . .\n",
      "(111, 6336)\n",
      "Scaling data . . .\n",
      "Done scaling data . . .\n"
     ]
    }
   ],
   "source": [
    "#TODO: reorganize and clean code up\n",
    "\n",
    "print('Loading data . . .')\n",
    "dataLoader = trafficDataLoader(1, 10)\n",
    "data = dataLoader.data\n",
    "print('Done loading data . . .')\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "# scale data using MinMaxScaler\n",
    "print('Scaling data . . .')\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "scaler_data = scaler.transform(data)\n",
    "print('Done scaling data . . .')\n",
    "\n",
    "data = np.transpose(scaler_data)  # [T, n]\n",
    "data = data[np.newaxis, :, :, np.newaxis]\n",
    "data = torch.from_numpy(data)                                           # [b, T, n, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference for opt: hardcoding to test and then make more dynamic later\n",
    "# batchSize = 1\n",
    "# nNode = len selSegs\n",
    "# dimFeature = 1\n",
    "# dimHidden = 25 (see big D in pt file)\n",
    "# interval = big T\n",
    "# useCuda\n",
    "\n",
    "opt = {\n",
    "    'batchSize': 1,\n",
    "    'nNode': 111, # update this later to read the number of rows in the selsegs file\n",
    "    'dimFeature': 1,\n",
    "    'dimHidden': 25, # read big D from the pt file\n",
    "    'truncate': 48, # read big T from pt file\n",
    "    'cuda': True,\n",
    "    'lr': .05, # read lr from pt file\n",
    "    'interval': data.size(1)\n",
    "}\n",
    "\n",
    "# convert the dictionary so we can use . syntax to reference its keys\n",
    "opt = SimpleNamespace(**opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model . . .\n",
      "Done loading model . . .\n"
     ]
    }
   ],
   "source": [
    "# load A and set 1 on the diagonal and zeros elsewhere of each node\n",
    "A = dataLoader.A\n",
    "# A = opt.alpha * A + np.eye(opt.nNode) # original\n",
    "A = np.eye(opt.nNode) # been training with this, no problem yet\n",
    "\n",
    "A = torch.from_numpy(A[np.newaxis, :, :])                               # [b, n, n]\n",
    "hState = torch.randn(opt.batchSize, opt.dimHidden, opt.nNode).double()  # [b, D, n]\n",
    "\n",
    "net = GRNN(opt)\n",
    "optimizer = optim.Adam(net.parameters(), lr=opt.lr)\n",
    "\n",
    "print('Loading model . . .')\n",
    "checkpoint = torch.load('./result/grnn29055-30int-1tid-0.0001a-48T-25D-4i-0.05lr-5487ms-1b-18sn.pt')\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "print('Done loading model . . .')\n",
    "\n",
    "# choose to run model in evaluation or training mode\n",
    "# set model to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# continue training the model\n",
    "# net.train()\n",
    "\n",
    "net.double()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if opt.cuda:\n",
    "    net.cuda()\n",
    "    criterion.cuda()\n",
    "    data = data.cuda()\n",
    "    A = A.cuda()\n",
    "    hState = hState.cuda()\n",
    "\n",
    "# use cuDNN along with cuda to train faster\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning predictions . . .\n"
     ]
    }
   ],
   "source": [
    "# start from the next index after 70% of the dataset is trained\n",
    "starting_index = int(opt.interval * .7) - opt.truncate\n",
    "ending_index = int(opt.interval) - opt.truncate\n",
    "\n",
    "predictions = np.zeros((opt.nNode, ending_index - starting_index))\n",
    "index = 0\n",
    "\n",
    "# starting index for checking accuracies\n",
    "reference_index = starting_index + opt.truncate\n",
    "\n",
    "print('Beginning predictions . . .')\n",
    "# for t in range(opt.interval - opt.truncate):\n",
    "for t in range(starting_index, ending_index, 1):\n",
    "    x = data[:, t:(t + opt.truncate), :, :] # batch, interval, node, feature\n",
    "    O, _ = net(x, hState, A) # data, hState, A\n",
    "\n",
    "    # rescale O before printing preditcions\n",
    "    transform_O_temp = np.zeros((opt.nNode, opt.interval))\n",
    "    transform_O_temp[:, :opt.truncate] = np.reshape(O[0, :, :, 0].cpu().data.numpy().flatten(), (opt.nNode, opt.truncate))\n",
    "    O = scaler.inverse_transform(transform_O_temp)[:, :opt.truncate]\n",
    "    \n",
    "    # store the predictions\n",
    "    predictions[:, index] = O[:, -1]\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111, 1901)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores: [86.95, 86.5, 87.02, 86.4, 86.31, 129.34, 129.26, 127.69, 128.23, 126.3, 124.79, 126.21, 125.61, 126.01, 125.01, 125.12, 125.35, 126.03, 126.18, 124.61, 127.0, 125.88, 127.29, 77.62, 77.59, 78.34, 77.1, 78.29, 78.5, 77.73, 78.17, 78.33, 77.13, 78.29, 78.64, 92.19, 92.55, 91.73, 92.68, 93.79, 91.86, 93.31, 93.78, 93.18, 94.01, 95.44, 94.12, 93.54, 94.92, 98.79, 95.82, 94.91, 98.11, 97.42, 98.71, 98.65, 99.94, 99.67, 98.33, 80.7, 81.47, 81.26, 82.37, 81.45, 81.65, 79.77, 81.95, 81.16, 81.13, 81.07, 82.89, 84.44, 85.01, 84.47, 83.96, 84.19, 83.82, 85.16, 84.39, 84.97, 84.6, 84.35, 83.97, 84.09, 84.09, 84.56, 85.16, 85.12, 84.61, 109.84, 110.15, 109.72, 109.83, 110.29, 109.25, 108.15, 107.2, 107.78, 107.83, 107.34, 106.54, 107.47, 107.35, 106.49, 107.07, 107.43, 105.45, 86.69, 86.45, 86.35, 87.08, 85.59, 85.06, 85.55, 85.12, 84.7, 84.25, 83.06, 84.15, 84.76, 84.96, 84.56, 85.56, 84.24, 83.96, 89.74, 90.24, 89.74, 88.99, 89.62, 89.07, 89.44, 89.19, 89.94, 90.09, 89.17, 90.41, 88.56, 89.51, 89.63, 90.19, 90.23, 89.5, 90.6, 91.35, 92.0, 91.63, 92.32, 92.8, 106.26, 106.38, 107.28, 106.45, 110.64, 112.0, 108.38, 108.0, 109.43, 110.11, 108.35, 109.29, 109.97, 108.74, 109.21, 110.07, 109.49, 109.66, 106.45, 107.8, 107.26, 106.96, 104.85, 104.35, 108.81, 110.66, 109.72, 107.17, 107.6, 107.05, 97.39, 98.24, 97.5, 96.77, 99.43, 97.52, 97.72, 97.02, 98.13, 99.48, 99.36, 99.21, 98.53, 97.89, 95.71, 97.39, 98.09, 99.43, 97.5, 95.2, 96.33, 96.92, 96.72, 97.64, 84.9, 84.46, 86.14, 86.76, 85.03, 86.08, 86.94, 85.12, 86.46, 85.57, 87.75, 86.42, 97.83, 96.48, 97.96, 99.3, 96.72, 99.17, 98.1, 98.68, 97.6, 98.32, 98.19, 98.5, 99.5, 99.37, 99.63, 99.53, 99.79, 99.39, 89.03, 89.07, 88.75, 89.82, 89.61, 88.81, 89.24, 88.99, 88.7, 88.59, 88.49, 88.4, 90.09, 89.67, 89.43, 89.39, 89.09, 88.75, 78.2, 78.11, 77.93, 77.7, 77.78, 77.98, 77.9, 78.27, 79.92, 77.86, 77.8, 79.95, 77.56, 77.85, 77.83, 77.95, 78.02, 77.85, 106.95, 107.1, 107.36, 107.58, 107.64, 107.97, 108.53, 108.49, 108.38, 108.71, 107.89, 107.36, 109.02, 109.04, 108.88, 109.02, 109.09, 109.12, 108.94, 108.79, 109.34, 109.14, 110.63, 109.13, 104.41, 104.89, 106.17, 107.46, 105.45, 105.58, 102.85, 104.6, 105.78, 104.13, 103.77, 102.69, 102.43, 103.63, 103.08, 102.6, 101.72, 101.77, 99.96, 101.8, 100.2, 101.25, 99.41, 99.77, 98.13, 98.51, 100.37, 97.09, 97.24, 98.03, 103.21, 103.43, 102.84, 102.35, 102.51, 102.73, 100.24, 101.76, 101.71, 100.82, 101.2, 102.09, 97.97, 101.05, 98.56, 98.26, 102.21, 99.15, 102.1, 100.62, 98.66, 100.67, 98.29, 100.52, 95.3, 96.58, 93.94, 95.25, 95.88, 95.4, 96.24, 96.03, 96.18, 93.97, 93.4, 92.82, 87.21, 87.64, 86.75, 87.15, 86.81, 87.02, 87.57, 86.3, 86.19, 85.64, 87.13, 87.62, 85.54, 86.17, 85.27, 85.78, 85.27, 86.48, 105.23, 104.62, 104.6, 104.85, 105.27, 104.37, 103.81, 103.64, 104.79, 104.41, 103.14, 103.39, 101.91, 102.56, 102.29, 103.91, 103.78, 103.66, 93.98, 96.06, 95.86, 94.86, 97.01, 96.78, 93.72, 96.54, 94.44, 94.89, 94.19, 92.62, 94.65, 96.77, 97.35, 96.25, 96.65, 95.19, 80.25, 81.6, 81.13, 81.21, 81.91, 82.05, 81.98, 82.17, 82.86, 83.16, 83.43, 81.14, 82.46, 82.32, 82.56, 82.77, 83.07, 84.86, 85.35, 84.6, 84.98, 85.47, 85.53, 86.25, 104.18, 103.79, 104.86, 107.6, 103.99, 105.34, 105.9, 106.29, 105.02, 106.1, 105.93, 106.47, 105.76, 106.4, 105.29, 105.76, 105.61, 105.74, 104.97, 103.65, 104.39, 103.89, 103.21, 103.69, 104.09, 104.08, 103.78, 103.2, 102.83, 102.81, 91.86, 91.29, 91.63, 91.22, 90.85, 90.8, 90.64, 91.05, 90.46, 90.29, 90.71, 90.76, 91.07, 91.27, 90.92, 92.27, 92.93, 92.15, 91.41, 93.92, 91.54, 90.8, 91.06, 90.95, 91.31, 92.26, 92.88, 93.08, 93.1, 93.57, 93.54, 93.34, 92.89, 92.61, 92.57, 91.68, 118.43, 118.39, 117.75, 118.33, 116.66, 117.25, 117.16, 117.54, 116.36, 116.5, 114.97, 114.72, 116.82, 112.93, 114.32, 117.23, 115.53, 115.78, 90.62, 90.2, 90.56, 89.41, 89.33, 88.68, 88.49, 88.58, 87.89, 87.13, 87.41, 86.48, 86.36, 86.83, 86.18, 84.3, 86.05, 85.6, 72.67, 72.55, 72.4, 72.04, 72.65, 72.65, 73.28, 73.6, 73.4, 72.35, 72.07, 73.55, 74.12, 73.87, 74.65, 74.39, 74.83, 75.0, 96.28, 96.88, 96.86, 98.02, 99.42, 100.72, 99.6, 99.87, 100.92, 100.7, 101.48, 101.9, 102.1, 101.39, 102.54, 102.27, 102.02, 105.16, 102.52, 104.88, 103.45, 103.53, 105.1, 105.25, 108.88, 108.74, 105.43, 107.51, 107.32, 103.54, 105.63, 104.21, 104.23, 105.45, 104.76, 103.69, 103.76, 103.53, 102.79, 102.57, 102.74, 101.72, 88.31, 87.56, 88.04, 89.83, 91.03, 89.52, 87.73, 87.92, 87.13, 88.67, 89.23, 88.42, 89.92, 89.75, 90.25, 89.85, 90.64, 89.86, 88.46, 88.41, 88.44, 89.09, 89.74, 89.6, 88.65, 91.57, 91.61, 89.05, 90.15, 89.62, 88.68, 88.25, 88.06, 87.08, 88.81, 89.42, 100.35, 99.68, 98.97, 98.1, 98.54, 99.51, 99.11, 99.85, 98.46, 98.55, 97.68, 95.4, 100.84, 100.39, 100.46, 100.58, 98.27, 98.36, 98.62, 98.45, 99.51, 99.68, 100.81, 97.35, 95.69, 95.76, 96.19, 96.56, 96.25, 96.19, 93.11, 93.86, 95.84, 94.19, 91.27, 95.71, 94.53, 93.4, 93.49, 93.97, 93.73, 93.24, 94.27, 93.94, 94.05, 94.75, 93.9, 91.75, 104.76, 104.03, 104.8, 105.96, 105.76, 105.34, 104.34, 104.03, 106.38, 104.46, 104.73, 105.55, 106.43, 104.4, 105.1, 104.31, 105.24, 104.78, 91.27, 91.21, 91.36, 91.46, 91.19, 90.18, 91.21, 90.44, 90.89, 90.2, 89.82, 90.55, 88.13, 88.41, 87.42, 87.56, 87.34, 87.09, 87.33, 87.3, 87.14, 86.61, 86.92, 87.11, 129.06, 128.29, 127.47, 127.49, 124.81, 124.49, 126.19, 125.6, 126.35, 126.28, 125.19, 125.39, 126.53, 124.48, 126.81, 125.56, 125.68, 125.9, 75.86, 76.81, 77.07, 75.64, 76.09, 78.28, 77.18, 77.61, 76.77, 78.45, 77.45, 79.09, 79.4, 78.15, 78.4, 79.75, 79.31, 79.23, 79.61, 79.57, 80.03, 79.83, 80.91, 79.9, 81.0, 81.32, 82.16, 80.9, 83.21, 82.32, 83.62, 85.45, 83.63, 85.54, 85.22, 85.58, 95.61, 95.96, 95.7, 96.54, 96.21, 98.32, 93.87, 95.64, 94.19, 94.68, 94.41, 95.28, 96.27, 96.81, 97.82, 96.49, 96.89, 97.86, 96.47, 97.31, 96.76, 98.02, 96.66, 97.54, 97.21, 97.32, 97.18, 97.12, 96.59, 96.09, 93.47, 93.55, 94.02, 93.43, 94.06, 93.67, 93.35, 94.88, 93.16, 93.69, 93.77, 93.65, 93.99, 94.74, 93.76, 94.61, 94.42, 93.02, 104.64, 102.77, 104.94, 105.6, 105.04, 104.75, 104.13, 104.34, 106.4, 105.95, 105.15, 106.25, 105.55, 105.42, 104.88, 104.88, 105.3, 104.73, 91.23, 91.28, 91.06, 90.95, 90.2, 90.0, 90.29, 90.94, 90.97, 90.43, 89.56, 91.46, 88.61, 88.93, 87.89, 87.09, 87.94, 87.39, 86.61, 87.31, 86.88, 86.13, 86.69, 86.75, 130.57, 128.06, 127.08, 126.63, 125.53, 125.61, 126.88, 125.76, 125.17, 125.5, 125.94, 125.11, 125.44, 126.05, 124.61, 127.14, 125.33, 126.01, 77.03, 77.24, 77.11, 76.02, 77.41, 78.0, 77.62, 77.49, 78.03, 77.38, 77.86, 77.24, 78.08, 78.88, 78.68, 78.45, 80.36, 80.54, 79.23, 78.31, 79.72, 80.49, 81.18, 80.71, 81.63, 80.15, 81.16, 82.34, 82.59, 83.34, 85.42, 82.23, 83.84, 85.69, 85.0, 84.97, 95.83, 95.32, 94.14, 96.06, 95.3, 95.98, 94.41, 94.53, 92.81, 93.88, 94.37, 95.43, 96.35, 97.3, 96.7, 98.06, 97.21, 94.83, 96.57, 97.23, 98.32, 97.13, 96.62, 96.52, 96.89, 96.55, 96.69, 96.74, 97.41, 96.26, 93.15, 93.42, 95.07, 93.65, 93.3, 93.61, 93.41, 94.18, 93.64, 93.41, 93.79, 93.76, 93.77, 94.7, 94.23, 94.67, 93.22, 93.68, 106.63, 104.1, 104.75, 103.64, 105.13, 105.56, 104.82, 104.02, 105.82, 105.11, 105.14, 106.34, 105.88, 105.23, 105.34, 105.27, 105.08, 104.74, 90.68, 91.01, 91.09, 91.0, 91.06, 90.29, 90.78, 90.81, 91.13, 90.12, 90.47, 91.28, 87.77, 88.03, 87.99, 87.13, 87.54, 87.23, 86.82, 86.83, 86.64, 86.8, 86.74, 86.68, 130.0, 129.08, 126.96, 127.63, 124.82, 125.05, 126.77, 126.19, 125.42, 125.47, 125.92, 126.33, 125.13, 125.8, 125.9, 125.95, 126.78, 125.97, 76.6, 78.02, 79.69, 77.81, 76.65, 77.35, 76.92, 77.92, 77.48, 80.3, 79.06, 79.45, 91.01, 92.05, 93.14, 90.68, 91.85, 92.1, 92.52, 94.07, 96.62, 93.31, 95.38, 93.39, 91.91, 93.25, 94.46, 95.06, 95.12, 94.71, 97.91, 96.19, 96.46, 100.06, 98.06, 97.69, 81.03, 80.77, 81.51, 83.13, 81.46, 81.14, 80.52, 81.65, 83.02, 84.46, 83.44, 81.89, 85.14, 84.39, 84.09, 85.17, 84.81, 84.26, 85.8, 84.49, 83.77, 85.47, 85.17, 84.53, 84.13, 84.6, 84.0, 85.24, 84.52, 84.25, 110.38, 110.68, 110.65, 110.3, 110.33, 109.56, 109.15, 107.78, 107.94, 108.03, 107.78, 106.45, 107.34, 106.53, 106.61, 106.53, 106.87, 106.83, 87.89, 87.35, 85.35, 86.87, 86.98, 86.63, 87.44, 84.69, 85.07, 84.07, 83.01, 84.3, 84.49, 85.79, 83.57, 83.37, 84.4, 83.95, 89.96, 89.78, 89.75, 89.61, 89.73, 89.37, 89.85, 89.66, 90.72, 89.97, 90.19, 89.92, 90.49, 90.2, 91.97, 90.36, 91.02, 89.59, 91.32, 91.91, 91.5, 92.12, 92.24, 92.95, 106.02, 106.77, 107.56, 108.24, 109.5, 110.69, 108.4, 107.94, 109.23, 108.4, 107.98, 108.43, 108.25, 109.03, 108.93, 109.55, 109.88, 108.34, 108.67, 107.18, 106.0, 106.82, 106.1, 105.03, 107.78, 107.68, 108.16, 108.14, 107.19, 106.95, 97.09, 98.45, 97.24, 97.04, 97.27, 99.68, 95.72, 96.71, 98.56, 97.42, 96.73, 97.6, 98.29, 97.93, 97.43, 96.08, 97.57, 97.26, 96.93, 96.64, 96.66, 97.11, 98.57, 97.13, 84.08, 85.06, 85.86, 84.82, 86.45, 84.65, 87.19, 85.62, 85.45, 85.65, 87.07, 86.19, 97.31, 97.67, 98.89, 98.74, 97.71, 98.79, 98.35, 98.47, 97.99, 98.99, 98.51, 98.78, 99.24, 100.22, 99.37, 99.37, 101.11, 98.83, 88.56, 88.99, 88.97, 89.27, 89.06, 88.73, 89.04, 88.97, 88.62, 88.75, 88.5, 88.9, 89.25, 88.7, 90.08, 88.95, 90.2, 90.7, 78.55, 79.11, 79.63, 77.91, 77.08, 78.04, 77.64, 77.55, 77.74, 78.45, 78.74, 79.9, 77.76, 77.87, 78.05, 77.95, 78.02, 77.84, 106.55, 107.08, 107.36, 109.74, 107.36, 106.31, 106.79, 111.51, 109.63, 111.16, 110.67, 108.63, 108.1, 110.4, 109.41, 109.3, 109.07, 109.59, 110.56, 108.65, 109.18, 111.18, 109.5, 111.34, 101.65, 102.94, 102.14, 105.52, 105.72, 103.32, 104.34, 102.58, 102.45, 104.12, 104.48, 103.78, 104.08, 101.15, 100.86, 100.63, 101.4, 101.27, 98.39, 99.25, 122.61, 97.68, 98.3, 99.42, 98.02, 96.15, 99.25, 96.62, 95.74, 97.06, 98.25, 104.54, 104.56, 102.47, 103.06, 102.32, 103.21, 106.32, 102.55, 101.44, 103.25, 102.01, 101.3, 103.87, 101.21, 100.75, 101.72, 102.15, 102.15, 100.6, 99.42, 100.83, 98.41, 99.11, 96.96, 95.89, 98.06, 97.64, 96.19, 97.02, 97.88, 95.17, 95.02, 95.28, 95.06, 95.66, 87.59, 88.26, 87.65, 87.86, 87.78, 87.64, 88.16, 87.67, 87.43, 87.18, 87.73, 88.55, 88.04, 87.04, 87.14, 87.29, 87.38, 87.44, 106.59, 107.11, 106.33, 106.39, 106.75, 106.76, 105.57, 106.02, 106.15, 106.78, 105.33, 105.24, 104.95, 104.77, 104.68, 105.11, 104.0, 104.91, 96.01, 96.33, 96.2, 96.55, 96.26, 95.67, 96.6, 95.84, 95.31, 94.68, 94.75, 95.19, 95.46, 96.22, 95.83, 95.41, 95.15, 95.16, 81.29, 81.46, 82.06, 81.3, 81.81, 81.6, 81.89, 82.35, 82.67, 83.04, 83.59, 82.13, 84.24, 82.46, 82.78, 82.61, 83.15, 83.88, 84.18, 84.41, 85.41, 84.43, 85.66, 86.71, 103.61, 103.66, 103.17, 105.4, 103.67, 104.07, 105.11, 104.86, 103.97, 105.33, 105.93, 105.1, 104.67, 105.36, 105.13, 105.12, 105.98, 105.46, 104.53, 104.26, 102.9, 102.76, 99.59, 102.43, 102.92, 105.45, 104.76, 102.31, 104.89, 101.78, 90.35, 90.53, 91.72, 92.2, 90.89, 90.15, 91.56, 91.44, 90.74, 90.83, 90.45, 90.64, 91.23, 91.98, 92.96, 90.25, 92.92, 94.09, 91.58, 91.18, 92.13, 90.23, 91.03, 91.13, 92.05, 93.64, 92.88, 92.42, 92.55, 93.57, 92.54, 92.6, 92.82, 92.96, 94.29, 91.54, 118.55, 117.52, 117.64, 117.59, 116.81, 118.05, 117.4, 116.85, 116.84, 117.59, 114.83, 113.66, 116.67, 116.51, 116.25, 116.47, 113.81, 113.09, 90.03, 89.81, 90.14, 89.86, 89.49, 87.98, 88.73, 88.01, 87.1, 86.26, 84.23, 86.35, 86.21, 86.57, 84.84, 84.72, 86.69, 85.87, 72.43, 72.41, 72.64, 72.38, 72.6, 72.43, 72.68, 73.86, 73.19, 73.27, 72.31, 73.1, 74.5, 74.85, 75.07, 75.31, 75.4, 74.53, 97.52, 97.4, 98.25, 99.34, 98.53, 98.94, 100.03, 99.38, 102.25, 100.75, 101.11, 100.47, 100.88, 101.25, 106.76, 105.25, 101.97, 103.44, 103.93, 103.6, 103.17, 105.48, 104.69, 105.0, 108.78, 108.21, 106.02, 107.67, 106.55, 103.04, 105.27, 105.06, 105.75, 106.15, 103.62, 105.26, 103.59, 104.09, 102.86, 103.79, 102.48, 102.31, 89.15, 88.62, 89.09, 90.67, 91.54, 89.49, 88.4, 87.64, 88.95, 88.55, 88.28, 89.6, 89.34, 91.04, 91.42, 90.75, 91.97, 90.06, 91.13, 90.68, 89.76, 90.32, 90.31, 89.65, 90.37, 90.31, 91.26, 89.06, 89.28, 89.94, 89.04, 88.35, 88.74, 88.89, 89.9, 88.85, 100.4, 99.22, 99.22, 99.3, 98.95, 99.68, 99.24, 99.77, 100.54, 98.51, 98.23, 95.73, 100.33, 100.69, 100.62, 100.7, 100.34, 100.03, 98.91, 99.13, 98.99, 99.74, 99.48, 99.54, 97.11, 95.14, 96.8, 96.26, 97.73, 96.49, 93.2, 93.63, 92.97, 93.21, 93.67, 93.7, 94.67, 93.89, 92.57, 93.75, 93.77, 93.63, 94.0, 94.09, 93.99, 94.67, 94.17, 92.68, 104.72, 103.83, 104.69, 105.04, 104.71, 104.84, 104.4, 104.18, 104.73, 106.53, 105.3, 106.16, 106.18, 105.46, 105.44, 105.36, 105.18, 104.93, 91.33, 91.1, 91.24, 90.05, 90.09, 90.36, 90.51, 90.51, 92.72, 90.32, 90.71, 90.26, 90.05, 87.99, 88.69, 87.49, 86.9, 88.39, 88.5, 91.06, 87.22, 86.72, 86.75, 86.86, 128.96, 130.48, 127.64, 129.24, 125.36, 124.52, 126.8, 125.82, 126.09, 126.66, 126.29, 124.73, 126.28, 127.5, 125.75, 125.32, 126.92, 128.62, 77.78, 77.32, 78.82, 77.91, 77.8, 77.94, 77.24, 78.24, 78.69, 78.34, 78.62, 78.31, 78.02, 79.0, 80.65, 78.83, 79.13, 80.22, 81.38, 78.3, 81.68, 81.18, 80.5, 81.64, 80.38, 82.73, 81.48, 81.93, 82.78, 82.53, 84.89, 84.24, 85.45, 85.43, 85.25, 85.25, 96.27, 95.6, 95.81, 96.81, 95.22, 96.44, 93.8, 95.51, 94.85, 93.74, 95.22, 95.61, 96.74, 96.35, 96.37, 97.18, 96.68, 96.85, 97.13, 97.48, 96.38, 98.2, 96.97, 98.16, 97.54, 95.14, 97.19, 96.89, 96.68, 96.37, 93.52, 93.49, 93.87, 93.81, 94.2, 95.22, 93.57, 93.46, 93.61, 93.83, 93.96, 93.28, 94.59, 94.78, 93.65, 94.69, 94.24, 91.58, 104.22, 104.46, 103.89, 105.38, 104.62, 104.9, 104.15, 103.74, 105.48, 106.01, 105.97, 105.76, 104.76, 105.42, 105.16, 105.45, 105.19, 105.21, 91.42, 91.36, 91.06, 91.54, 90.55, 90.18, 90.63, 91.23, 90.55, 92.38, 92.81, 92.35, 87.97, 88.81, 89.03, 87.06, 87.68, 88.38, 88.11, 87.31, 87.22, 87.01, 86.7, 87.46, 130.29, 129.14, 127.7, 127.23, 126.41, 124.92, 126.97, 126.1, 126.21, 125.93, 125.92, 126.53, 127.14, 126.0, 125.88, 126.15, 125.69, 125.79, 76.81, 77.53, 77.32, 76.38, 79.54, 79.4]\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for j in range(0, index, 1):\n",
    "    accuracy_score = 0\n",
    "    test_comparison_data = dataLoader.data[:, reference_index + j]\n",
    "    \n",
    "    for i in range(opt.nNode):\n",
    "        # give an accuracy of 0 if the test comparison data is 0\n",
    "        if test_comparison_data[i] == 0:\n",
    "            accuracy_score += 0\n",
    "        else:\n",
    "            accuracy_score += predictions[i, j] / test_comparison_data[i]\n",
    "\n",
    "    accuracy_score /= opt.nNode\n",
    "    accuracies.append(round(accuracy_score * 100.0, 2))\n",
    "\n",
    "\n",
    "print(f'Accuracy scores: {accuracies}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "average_accuracy = reduce(lambda x, y: x + y, accuracies) / len(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.12988953182551"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
